{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9877b7-b39e-40b2-8caf-57a3a4dfe133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3200a71-78d5-455b-85ef-af1ed5834d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.12 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "from lib.dataloader import RSNA24DF\n",
    "\n",
    "from lib.classification.dataloader import ClassificationDataLoader, DiscLevelLocs\n",
    "from lib.patientInfo import DiscLevel, PatientInfo, Scan, Condition, Img, ImgLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc849e1-ed22-4ae2-9191-a96001ccfdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AXIAL_HEIGHT=256\n",
    "AXIAL_WIDTH=256\n",
    "\n",
    "SAGITTAL_HEIGHT = 256\n",
    "SAGITTAL_WIDHT = 256\n",
    "\n",
    "MODEL_NAME = \"tf_efficientnet_b5.ns_jft_in1k\"\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "DEPTH=8\n",
    "\n",
    "OUTPUT_DIM = 15\n",
    "N_CLASSES = 5\n",
    "\n",
    "WINDOW_HEIGHT_MM = 30\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "N_FOLDS = 5\n",
    "SEED = 41\n",
    "N_WORKERS = 8\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "GRAD_ACC = 2\n",
    "TGT_BATCH_SIZE = 32\n",
    "BATCH_SIZE = TGT_BATCH_SIZE // GRAD_ACC\n",
    "EARLY_STOPPING_EPOCH = 6\n",
    "OUTPUT_DIR = \"../models/classification/\"\n",
    "\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "N_FOLDS = 5\n",
    "AUG_PROB = 0.75\n",
    "\n",
    "LR = 2e-4 * TGT_BATCH_SIZE / 32\n",
    "AUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5245dc1-25d5-4b91-9f56-2ba7d576fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "train_df = train_df[train_df[\"study_id\"] != 3008676218]\n",
    "train_label_coordinates_df = pd.read_csv(f\"{DATA_DIR}/train_label_coordinates.csv\")\n",
    "train_series_descriptions_df = pd.read_csv(f\"{DATA_DIR}/train_series_descriptions.csv\")\n",
    "\n",
    "\n",
    "rsna24DF = RSNA24DF(train_df, train_label_coordinates_df, train_series_descriptions_df, f\"{DATA_DIR}/train_images\")\n",
    "\n",
    "\n",
    "# test_series_descriptions_df = pd.read_csv(f\"{DATA_DIR}/test_series_descriptions.csv\")\n",
    "\n",
    "# rsna24DF = RSNA24DF(None, None, test_series_descriptions_df, f\"{DATA_DIR}/test_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44ed097-b325-4078-a05b-ae6193e18bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalDataLoader(ClassificationDataLoader):\n",
    "    def __init__(self\n",
    "                , patient_ids\n",
    "                , phase\n",
    "                , transformations=None\n",
    "                , axial_height = AXIAL_HEIGHT\n",
    "                , axial_width = AXIAL_WIDTH\n",
    "                , sagittal_height = SAGITTAL_HEIGHT\n",
    "                , sagittal_width = SAGITTAL_WIDHT\n",
    "                , rsna24DF=rsna24DF\n",
    "                , disc_level_locs_dir=\"../processed-data\"\n",
    "                , depth=DEPTH\n",
    "                ) -> None:\n",
    "        super().__init__(patient_ids=patient_ids \n",
    "                         , rsna24DF=rsna24DF\n",
    "                         , transformations=transformations\n",
    "                         , depth=depth \n",
    "                         , disc_level_locs_dir=disc_level_locs_dir\n",
    "                         , phase=\"train\"\n",
    "                        )\n",
    "        self.axial_height = axial_height\n",
    "        self.axial_width = axial_width\n",
    "        self.sagittal_height = sagittal_height\n",
    "        self.sagittal_width = sagittal_width\n",
    "\n",
    "    def _axial_t2_imgs(self, disc_level: DiscLevel, patient_info: PatientInfo, disc_level_locs: DiscLevelLocs) -> np.ndarray:\n",
    "\n",
    "        # print(\"Sampling sagittal Axial IMGS\")\n",
    "        xs = np.zeros((self.axial_height, self.axial_width, self.depth))\n",
    "\n",
    "        level_mm = disc_level_locs.disc_loc_mm[disc_level.to_int()]\n",
    "        # print(f\"{level_mm=}\")\n",
    "\n",
    "        near_imgs = []\n",
    "\n",
    "        for img in patient_info.get_scans(Scan.AxialT2):\n",
    "            if abs(img.dicom.ImagePositionPatient[2] - level_mm) <= WINDOW_HEIGHT_MM / 2:\n",
    "                near_imgs.append(img)\n",
    "\n",
    "        # select top 4 near imgs\n",
    "        # print(f\"{len(near_imgs)=}\")\n",
    "        near_imgs = sorted(near_imgs, key = lambda img: abs(img.dicom.ImagePositionPatient[2] - level_mm))[:self.depth]\n",
    "\n",
    "        # sort these images from head to toe order\n",
    "        near_imgs = sorted(near_imgs, key = lambda img: img.dicom.ImagePositionPatient[2])\n",
    "        near_imgs.reverse()\n",
    "\n",
    "        for idx, img in enumerate(near_imgs):\n",
    "            # print(f\"{img.dicom.ImagePositionPatient[2]=}\")\n",
    "            pixels = img.dicom.pixel_array\n",
    "            xs[...,idx] = cv2.resize(pixels, (self.axial_height, self.axial_width),interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def _sagittal_t2_stir_imgs(self, disc_level: DiscLevel, patient_info: PatientInfo, disc_level_locs: DiscLevelLocs) -> np.ndarray:\n",
    "        # print(\"\\n\\n\\n\\n\")\n",
    "        # print(\"Sampling sagittal T2/STIR IMGS\")\n",
    "        return self._sagittal_imgs(Scan.SagittalT2_STIR, disc_level, patient_info, disc_level_locs)\n",
    "\n",
    "    def _sagittal_t1_imgs(self, disc_level: DiscLevel, patient_info: PatientInfo, disc_level_locs: DiscLevelLocs) -> np.ndarray:\n",
    "        # print(\"\\n\\n\\n\\n\")\n",
    "        # print(\"Sampling sagittal T1 IMGS\")\n",
    "        return self._sagittal_imgs(Scan.SagittalT1, disc_level, patient_info, disc_level_locs)\n",
    "\n",
    "    def _sagittal_imgs(self, scan_type: Scan, disc_level: DiscLevel, patient_info: PatientInfo, disc_level_locs: DiscLevelLocs) -> np.ndarray:\n",
    "        \n",
    "        xs = np.zeros((self.sagittal_height, self.sagittal_width, self.depth))\n",
    "        level_mm = disc_level_locs.disc_loc_mm[disc_level.to_int()]\n",
    "        # print(f\"{level_mm=}\")\n",
    "\n",
    "        imgs = sorted([(img.dicom.ImagePositionPatient[0], img) for img in patient_info.get_scans(scan_type)], key = lambda x: x[0])\n",
    "\n",
    "        if len(imgs) == 0:\n",
    "            return xs\n",
    "        \n",
    "        img_idxs = np.unique(np.round(np.linspace(0, len(imgs) - 1, self.depth)).astype(int))\n",
    "\n",
    "\n",
    "        for idx,img_idx in enumerate(img_idxs):\n",
    "\n",
    "            # try:\n",
    "            _,img = imgs[img_idx]\n",
    "            # except Exception as e:\n",
    "            #     print(f\"{patient_info.patient_id=}\")\n",
    "            #     print(f\"{img_idxs=}, {len(imgs)=}\")\n",
    "            \n",
    "\n",
    "            # print(f\"{img.dicom.ImagePositionPatient[0]=}\")\n",
    "\n",
    "            # crop image (width)\n",
    "            pixels = img.dicom.pixel_array\n",
    "            pixels = cv2.normalize(pixels, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "            _, binary_image = cv2.threshold(pixels, 50, 255, cv2.THRESH_BINARY)  \n",
    "            contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contour_areas = [cv2.contourArea(contour) for contour in contours]\n",
    "            largest_contour_index = np.argmax(contour_areas)\n",
    "            x, y, w, h = cv2.boundingRect(contours[largest_contour_index])\n",
    "            cropped_img = img.dicom.pixel_array[:, x:x+w]\n",
    "            # print(f\"{cropped_img.shape=} (before center cropping)\")\n",
    "            \n",
    "            window_width_px = WINDOW_HEIGHT_MM / img.dicom.PixelSpacing[0]\n",
    "            level_y_coord = (img.dicom.ImagePositionPatient[2] - level_mm) / img.dicom.PixelSpacing[0]\n",
    "            # assert (int(level_y_coord - window_width_px / 2) > 0)\n",
    "\n",
    "            # print(f\"center cropping range: {int(level_y_coord - window_width_px / 2)} to {int(level_y_coord + window_width_px / 2)} \")\n",
    "            cropped_img = cropped_img[max(int(level_y_coord - window_width_px / 2), 0) : int(level_y_coord + window_width_px / 2) , :]\n",
    "            \n",
    "            # print(f\"{cropped_img.shape=} (after center cropping)\")\n",
    "            \n",
    "            xs[..., idx] =  cv2.resize(cropped_img, (self.sagittal_height, self.sagittal_width),interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        return xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178c4f2c-2585-43d1-9d10-547ecb699728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    # def __init__(self, output_dim=OUTPUT_DIM, n_classes=N_CLASSES, model_name=MODEL_NAME):\n",
    "    #     super().__init__()\n",
    "        \n",
    "    #     self.initial_conv = nn.Conv3d(in_channels=1, out_channels=3, kernel_size=3, padding=1, stride=1)\n",
    "    #     self.output_dim = output_dim\n",
    "    #     self.n_classes = n_classes\n",
    "    #     model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)\n",
    "    #     self.feature_extractors = nn.ModuleList(\n",
    "    #         [model.blocks[i] for i in range(len(model.blocks) - 1) ]\n",
    "    #     )\n",
    "    #     self.final_pool = nn.Sequential(\n",
    "    #        nn.AvgPool3d(kernel_size=(8,7,7), stride=(1,1,1), padding=(0,0,0)), \n",
    "    #     nn.AdaptiveAvgPool3d(output_size=1)\n",
    "    #     )\n",
    "    #     self.classification_head = nn.Sequential(\n",
    "    #        nn.Dropout(p=0.5, inplace=False),\n",
    "    #        nn.Linear(2048, self.output_dim),\n",
    "    #     )\n",
    "\n",
    "    #     del model\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.initial_conv(x)\n",
    "    #     for feature_extractor in self.feature_extractors:\n",
    "    #         x = feature_extractor(x)\n",
    "    #     x = self.final_pool(x)\n",
    "    #     x = self.classification_head(x.squeeze())\n",
    "    #     return x\n",
    "        \n",
    "    def __init__(self, model_name, in_c=DEPTH * 3, n_classes=15, pretrained=True, features_only=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "                                    model_name,\n",
    "                                    pretrained=pretrained, \n",
    "                                    features_only=features_only,\n",
    "                                    in_chans=in_c,\n",
    "                                    num_classes=n_classes,\n",
    "                                    global_pool='avg'\n",
    "                                    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce63093e-dcf8-4ec3-8863-962e297f78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f498768-fcf4-4b4c-af0d-ae880068afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = A.Compose([\n",
    "    # A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=AUG_PROB),\n",
    "    # A.OneOf([\n",
    "    #     A.OpticalDistortion(distort_limit=1.0),\n",
    "    #     A.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "    #     A.ElasticTransform(alpha=3),\n",
    "    # ], p=AUG_PROB),\n",
    "\n",
    "    # A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=AUG_PROB),\n",
    "    # A.Resize(AXIAL_HEIGHT, AXIAL_WIDTH),\n",
    "    # A.CoarseDropout(max_holes=16, max_height=64, max_width=64, min_holes=1, min_height=8, min_width=8, p=AUG_PROB),    \n",
    "    A.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "transforms_val = A.Compose([\n",
    "    # A.Resize(AXIAL_HEIGHT, AXIAL_WIDTH),\n",
    "    A.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dde0646-c63a-4521-808b-87ac810965e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,dataloader, criterion, optimizer, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    training_loss = []\n",
    "    \n",
    "    for idx, (x, y) in enumerate(pbar := tqdm(dataloader)):\n",
    "        \n",
    "        x, y = x.to(DEVICE), y.long().to(DEVICE)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            pred_y = model(x)\n",
    "            pred_y = pred_y.reshape(-1, 3)\n",
    "            loss = criterion(pred_y, y.reshape(-1))\n",
    "            loss = loss / GRAD_ACC\n",
    "\n",
    "            \n",
    "        loss.backward()\n",
    "        norm = nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "\n",
    "        training_loss.append(loss.item() * GRAD_ACC)\n",
    "\n",
    "        if (idx + 1) % GRAD_ACC == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "        pbar.set_description(f\"loss: {loss.item() * GRAD_ACC:.6f}, | norm: {norm:.4f}| training_loss: {np.mean(training_loss):.6f}\")\n",
    "        \n",
    "    \n",
    "    return np.mean(training_loss)\n",
    "\n",
    "def validation(model, data_loader, criterion):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for idx, (x, y) in enumerate(pbar := tqdm(data_loader)):\n",
    "                \n",
    "                x, y = x.to(DEVICE), y.long().to(DEVICE)\n",
    "\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    \n",
    "                    pred_y = model(x)\n",
    "                    pred_y = pred_y.reshape(-1, 3)\n",
    "                    loss = criterion(pred_y, y.reshape(-1))\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "                pbar.set_description(f\"current loss: {loss.item():.6f}, validation_loss: {np.mean(val_loss):.6f}\")\n",
    "\n",
    "        return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5de462c-5569-47cb-8c23-e9f5cd7264a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b5.ns_jft_in1k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "start fold0\n",
      "##############################\n",
      "1579 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._hub:[timm/tf_efficientnet_b5.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Converted input conv conv_stem pretrained weights from 3 to 24 channel(s)\n",
      "INFO:timm.models._builder:Missing keys (classifier.weight, classifier.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                       | 0/98 [00:00<?, ?it/s]/tmp/ipykernel_130634/1354135051.py:19: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  norm = nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
      "loss: 1.070396, | norm: 6.0117| training_loss: 1.361337: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.83it/s]\n",
      "current loss: 1.237676, validation_loss: 1.266427: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.2664\n",
      "updating best_val_loss from 20.902649 to 1.2664265999427209\n",
      "updated losses: best_val_loss=1.2664265999427209\n",
      "Saving model....\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.842423, | norm: 3.1346| training_loss: 0.980758: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.71it/s]\n",
      "current loss: 0.789191, validation_loss: 1.010502: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 1.0105\n",
      "updating best_val_loss from 1.2664265999427209 to 1.010501884497129\n",
      "updated losses: best_val_loss=1.010501884497129\n",
      "Saving model....\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.654059, | norm: 1.8931| training_loss: 0.829734: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.87it/s]\n",
      "current loss: 0.659258, validation_loss: 0.834524: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8345\n",
      "updating best_val_loss from 1.010501884497129 to 0.8345239001971024\n",
      "updated losses: best_val_loss=0.8345239001971024\n",
      "Saving model....\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.106116, | norm: 3.6012| training_loss: 0.780106: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.83it/s]\n",
      "current loss: 0.980719, validation_loss: 0.836258: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8363\n",
      "5 more epochs to train until early stopping\n",
      "EPOCH: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.619677, | norm: 1.5873| training_loss: 0.744581: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.79it/s]\n",
      "current loss: 0.848104, validation_loss: 0.797468: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7975\n",
      "updating best_val_loss from 0.8345239001971024 to 0.7974684605231652\n",
      "updated losses: best_val_loss=0.7974684605231652\n",
      "Saving model....\n",
      "EPOCH: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.646602, | norm: 2.0413| training_loss: 0.707565: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.81it/s]\n",
      "current loss: 0.667565, validation_loss: 0.857785: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8578\n",
      "5 more epochs to train until early stopping\n",
      "EPOCH: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.768049, | norm: 2.2335| training_loss: 0.668065: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.68it/s]\n",
      "current loss: 0.599330, validation_loss: 0.851151: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8512\n",
      "4 more epochs to train until early stopping\n",
      "EPOCH: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.800804, | norm: 2.6480| training_loss: 0.615684: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.75it/s]\n",
      "current loss: 0.600349, validation_loss: 0.897478: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:07<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8975\n",
      "3 more epochs to train until early stopping\n",
      "EPOCH: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.657627, | norm: 2.6590| training_loss: 0.587942: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.83it/s]\n",
      "current loss: 0.565228, validation_loss: 0.872453: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8725\n",
      "2 more epochs to train until early stopping\n",
      "EPOCH: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.635659, | norm: 2.8735| training_loss: 0.529170: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.76it/s]\n",
      "current loss: 0.553116, validation_loss: 0.980774: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.9808\n",
      "1 more epochs to train until early stopping\n",
      "EPOCH: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.302529, | norm: 1.9913| training_loss: 0.465565: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.75it/s]\n",
      "current loss: 0.815569, validation_loss: 0.880021: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:08<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8800\n",
      "0 more epochs to train until early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(range(len(train_df)))):\n",
    "    print('#'*30)\n",
    "    print(f'start fold{fold}')\n",
    "    print('#'*30)\n",
    "    print(len(trn_idx), len(val_idx))\n",
    "\n",
    "    df_train = train_df.iloc[trn_idx]\n",
    "    df_valid = train_df.iloc[val_idx]\n",
    "\n",
    "\n",
    "    train_ds = FinalDataLoader(df_train[\"study_id\"].unique(), transformations=transforms_train, phase=\"train\")\n",
    "    train_dl = DataLoader(\n",
    "                train_ds,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                drop_last=True,\n",
    "                num_workers=N_WORKERS\n",
    "                )\n",
    "    \n",
    "    valid_ds = FinalDataLoader(df_valid[\"study_id\"].unique(), transformations=transforms_val, phase=\"train\")\n",
    "    valid_dl = DataLoader(\n",
    "                valid_ds,\n",
    "                batch_size=BATCH_SIZE * 2,\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "                drop_last=False,\n",
    "                num_workers=N_WORKERS\n",
    "                )\n",
    "    \n",
    "    model = ClassificationModel(model_name=MODEL_NAME).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    \n",
    "    weights = torch.tensor([1.0, 2.0, 4.0])\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE))\n",
    "    \n",
    "    warmup_steps = EPOCHS/10 * len(train_dl) // GRAD_ACC\n",
    "    num_total_steps = EPOCHS * len(train_dl) // GRAD_ACC\n",
    "    num_cycles = 0.475\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=num_total_steps,\n",
    "                                                num_cycles=num_cycles)\n",
    "\n",
    "    \n",
    "    best_val_loss   = 20.902649\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"EPOCH: {epoch}\")\n",
    "        _ = train(model, train_dl, criterion, optimizer, scheduler)\n",
    "        val_loss = validation(model, valid_dl, criterion)\n",
    "\n",
    "        print(f\"val_loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            early_stop_counter = 0\n",
    "            print(f\"updating best_val_loss from {best_val_loss} to {val_loss}\")\n",
    "            \n",
    "            best_val_loss = val_loss\n",
    "\n",
    "            print(f\"updated losses: {best_val_loss=}\")\n",
    "\n",
    "            print(\"Saving model....\")\n",
    "            fname = f'../models/classification//best_loc_model_fold-{fold}.pt'\n",
    "            torch.save(model.state_dict(), fname)\n",
    "            \n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"{EARLY_STOPPING_EPOCH - early_stop_counter} more epochs to train until early stopping\")\n",
    "\n",
    "        if early_stop_counter == EARLY_STOPPING_EPOCH:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcda5c-c4e2-423f-bf74-759f55e66d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"EPOCH: {epoch}\")\n",
    "    _ = train(model, train_dl, criterion, optimizer, scheduler)\n",
    "    val_loss = validation(model, valid_dl, criterion)\n",
    "\n",
    "    print(f\"val_loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        early_stop_counter = 0\n",
    "        print(f\"updating best_val_loss from {best_val_loss} to {val_loss}\")\n",
    "        \n",
    "        best_val_loss = val_loss\n",
    "\n",
    "        print(f\"updated losses: {best_val_loss=}\")\n",
    "\n",
    "        print(\"Saving model....\")\n",
    "        fname = f'../models/classification//best_loc_model_fold-{fold}.pt'\n",
    "        torch.save(model.state_dict(), fname)\n",
    "        \n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"{EARLY_STOPPING_EPOCH - early_stop_counter} more epochs to train until early stopping\")\n",
    "\n",
    "    if early_stop_counter == EARLY_STOPPING_EPOCH:\n",
    "        break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63d218cc-640c-46b7-978b-f5458725a073",
   "metadata": {},
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "    training_loss = []\n",
    "\n",
    "    for batch_start_idx in range(0, train_xs.shape[0], BATCH_SIZE):\n",
    "        batch_xs, batch_ys = train_xs[batch_start_idx: batch_start_idx + BATCH_SIZE], train_ys[batch_start_idx: batch_start_idx + BATCH_SIZE]\n",
    "        batch_xs = batch_xs.to(DEVICE)\n",
    "        batch_ys = batch_ys.to(DEVICE)\n",
    "        batch_xs = batch_xs.unsqueeze(1)\n",
    "        \n",
    "        pred_ys = model(batch_xs)\n",
    "        pred_ys = pred_ys.reshape(-1, 3)\n",
    "\n",
    "        loss = criterion(pred_ys, batch_ys.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss.append(loss.item())\n",
    "\n",
    "    print(f\"epoch: {epoch}, loss: {np.mean(training_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "178f65ef-73ce-4a23-b1d2-8dd29d67f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/paradox/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ClassificationModel(model_name=MODEL_NAME).to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(\"../models/classification/best_loc_model_fold-0.pt\"))\n",
    "model.eval()\n",
    "\n",
    "ds = FinalDataLoader(test_series_descriptions_df[\"study_id\"].unique(), transformations=transforms_val, phase=\"pred\", disc_level_locs_dir=\"../processed-data/test-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f60f646-09d8-49e7-ba21-ba8023519c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level_mm=62.20833\n",
      "level_mm=62.20833\n",
      "level_mm=62.20833\n",
      "tensor([ 0.4057, -0.4424, -0.0332,  0.0242,  0.2285, -0.3126, -0.0453,  0.1302,\n",
      "        -0.2160, -0.1779,  0.0373,  0.1771, -0.1856,  0.0184,  0.3195],\n",
      "       device='cuda:0')\n",
      "level_mm=34.433823\n",
      "level_mm=34.433823\n",
      "level_mm=34.433823\n",
      "tensor([ 0.3217, -0.3847, -0.0195, -0.0971,  0.2153, -0.1836, -0.1216,  0.1495,\n",
      "        -0.0982, -0.1911,  0.0737,  0.2167, -0.2191,  0.0416,  0.3105],\n",
      "       device='cuda:0')\n",
      "level_mm=20.595108\n",
      "level_mm=20.595108\n",
      "level_mm=20.595108\n",
      "tensor([ 0.3081, -0.4020,  0.0328, -0.1800,  0.2407, -0.1573, -0.2032,  0.1702,\n",
      "        -0.0549, -0.2865,  0.1053,  0.2727, -0.3081,  0.0647,  0.3911],\n",
      "       device='cuda:0')\n",
      "level_mm=0.5409241\n",
      "level_mm=0.5409241\n",
      "level_mm=0.5409241\n",
      "tensor([ 0.2847, -0.4143,  0.0322, -0.2370,  0.2510, -0.1299, -0.2485,  0.1846,\n",
      "        -0.0375, -0.3041,  0.1014,  0.3077, -0.3226,  0.0703,  0.4030],\n",
      "       device='cuda:0')\n",
      "level_mm=-20.475632\n",
      "level_mm=-20.475632\n",
      "level_mm=-20.475632\n",
      "tensor([ 0.3266, -0.3874, -0.0233, -0.1876,  0.1899, -0.1279, -0.1832,  0.1688,\n",
      "        -0.0353, -0.1947,  0.1269,  0.2269, -0.2437,  0.0440,  0.3086],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = []\n",
    "for x in ds[0]:\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(x).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        y = model(x)\n",
    "        print(y)\n",
    "        y = y.reshape(-1, 3).softmax(dim=1).cpu().numpy()\n",
    "        preds.append(y.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b4f625d-0554-4d33-b155-23c0e4ad26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "\n",
    "for i in range(5):\n",
    "    disc_level = DiscLevel.from_int(i)\n",
    "    disc_level_preds = preds[i]\n",
    "    for condition, pred in zip(Condition.all_conditions(), disc_level_preds):\n",
    "        loc = Location(disc_level=disc_level, condition=condition).to_str()\n",
    "        df[f\"44036939_{loc}\"] = {\"normal_mild\": pred[0], \"moderate\": pred[1], \"severe\": pred[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59a28fa1-a274-49b6-a13f-1d329c0632f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.patientInfo import Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34b6d43b-a19e-46bd-9fea-a53d109d7dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal_mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44036939_spinal_canal_stenosis_l1_l2</th>\n",
       "      <td>0.482407</td>\n",
       "      <td>0.206579</td>\n",
       "      <td>0.311014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_neural_foraminal_narrowing_l1_l2</th>\n",
       "      <td>0.340065</td>\n",
       "      <td>0.417134</td>\n",
       "      <td>0.242801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_neural_foraminal_narrowing_l1_l2</th>\n",
       "      <td>0.329501</td>\n",
       "      <td>0.392703</td>\n",
       "      <td>0.277795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_subarticular_stenosis_l1_l2</th>\n",
       "      <td>0.272749</td>\n",
       "      <td>0.338263</td>\n",
       "      <td>0.388988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_subarticular_stenosis_l1_l2</th>\n",
       "      <td>0.257492</td>\n",
       "      <td>0.315776</td>\n",
       "      <td>0.426732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_spinal_canal_stenosis_l2_l3</th>\n",
       "      <td>0.453645</td>\n",
       "      <td>0.223834</td>\n",
       "      <td>0.322521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_neural_foraminal_narrowing_l2_l3</th>\n",
       "      <td>0.304526</td>\n",
       "      <td>0.416180</td>\n",
       "      <td>0.279294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_neural_foraminal_narrowing_l2_l3</th>\n",
       "      <td>0.299836</td>\n",
       "      <td>0.393211</td>\n",
       "      <td>0.306953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_subarticular_stenosis_l2_l3</th>\n",
       "      <td>0.262683</td>\n",
       "      <td>0.342335</td>\n",
       "      <td>0.394982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_subarticular_stenosis_l2_l3</th>\n",
       "      <td>0.250241</td>\n",
       "      <td>0.324781</td>\n",
       "      <td>0.424978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_spinal_canal_stenosis_l3_l4</th>\n",
       "      <td>0.444257</td>\n",
       "      <td>0.218402</td>\n",
       "      <td>0.337341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_neural_foraminal_narrowing_l3_l4</th>\n",
       "      <td>0.282017</td>\n",
       "      <td>0.429501</td>\n",
       "      <td>0.288482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_neural_foraminal_narrowing_l3_l4</th>\n",
       "      <td>0.276815</td>\n",
       "      <td>0.402126</td>\n",
       "      <td>0.321059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_subarticular_stenosis_l3_l4</th>\n",
       "      <td>0.236462</td>\n",
       "      <td>0.349890</td>\n",
       "      <td>0.413648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_subarticular_stenosis_l3_l4</th>\n",
       "      <td>0.224013</td>\n",
       "      <td>0.325238</td>\n",
       "      <td>0.450749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_spinal_canal_stenosis_l4_l5</th>\n",
       "      <td>0.439767</td>\n",
       "      <td>0.218593</td>\n",
       "      <td>0.341640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_neural_foraminal_narrowing_l4_l5</th>\n",
       "      <td>0.267240</td>\n",
       "      <td>0.435324</td>\n",
       "      <td>0.297436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_neural_foraminal_narrowing_l4_l5</th>\n",
       "      <td>0.264773</td>\n",
       "      <td>0.408278</td>\n",
       "      <td>0.326949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_subarticular_stenosis_l4_l5</th>\n",
       "      <td>0.230209</td>\n",
       "      <td>0.345323</td>\n",
       "      <td>0.424469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_subarticular_stenosis_l4_l5</th>\n",
       "      <td>0.219901</td>\n",
       "      <td>0.325762</td>\n",
       "      <td>0.454337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_spinal_canal_stenosis_l5_s1</th>\n",
       "      <td>0.455709</td>\n",
       "      <td>0.223153</td>\n",
       "      <td>0.321138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_neural_foraminal_narrowing_l5_s1</th>\n",
       "      <td>0.284075</td>\n",
       "      <td>0.414379</td>\n",
       "      <td>0.301547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_neural_foraminal_narrowing_l5_s1</th>\n",
       "      <td>0.279232</td>\n",
       "      <td>0.397025</td>\n",
       "      <td>0.323742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_left_subarticular_stenosis_l5_s1</th>\n",
       "      <td>0.256163</td>\n",
       "      <td>0.353339</td>\n",
       "      <td>0.390498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44036939_right_subarticular_stenosis_l5_s1</th>\n",
       "      <td>0.245679</td>\n",
       "      <td>0.327552</td>\n",
       "      <td>0.426770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 normal_mild  moderate  \\\n",
       "44036939_spinal_canal_stenosis_l1_l2                0.482407  0.206579   \n",
       "44036939_left_neural_foraminal_narrowing_l1_l2      0.340065  0.417134   \n",
       "44036939_right_neural_foraminal_narrowing_l1_l2     0.329501  0.392703   \n",
       "44036939_left_subarticular_stenosis_l1_l2           0.272749  0.338263   \n",
       "44036939_right_subarticular_stenosis_l1_l2          0.257492  0.315776   \n",
       "44036939_spinal_canal_stenosis_l2_l3                0.453645  0.223834   \n",
       "44036939_left_neural_foraminal_narrowing_l2_l3      0.304526  0.416180   \n",
       "44036939_right_neural_foraminal_narrowing_l2_l3     0.299836  0.393211   \n",
       "44036939_left_subarticular_stenosis_l2_l3           0.262683  0.342335   \n",
       "44036939_right_subarticular_stenosis_l2_l3          0.250241  0.324781   \n",
       "44036939_spinal_canal_stenosis_l3_l4                0.444257  0.218402   \n",
       "44036939_left_neural_foraminal_narrowing_l3_l4      0.282017  0.429501   \n",
       "44036939_right_neural_foraminal_narrowing_l3_l4     0.276815  0.402126   \n",
       "44036939_left_subarticular_stenosis_l3_l4           0.236462  0.349890   \n",
       "44036939_right_subarticular_stenosis_l3_l4          0.224013  0.325238   \n",
       "44036939_spinal_canal_stenosis_l4_l5                0.439767  0.218593   \n",
       "44036939_left_neural_foraminal_narrowing_l4_l5      0.267240  0.435324   \n",
       "44036939_right_neural_foraminal_narrowing_l4_l5     0.264773  0.408278   \n",
       "44036939_left_subarticular_stenosis_l4_l5           0.230209  0.345323   \n",
       "44036939_right_subarticular_stenosis_l4_l5          0.219901  0.325762   \n",
       "44036939_spinal_canal_stenosis_l5_s1                0.455709  0.223153   \n",
       "44036939_left_neural_foraminal_narrowing_l5_s1      0.284075  0.414379   \n",
       "44036939_right_neural_foraminal_narrowing_l5_s1     0.279232  0.397025   \n",
       "44036939_left_subarticular_stenosis_l5_s1           0.256163  0.353339   \n",
       "44036939_right_subarticular_stenosis_l5_s1          0.245679  0.327552   \n",
       "\n",
       "                                                   severe  \n",
       "44036939_spinal_canal_stenosis_l1_l2             0.311014  \n",
       "44036939_left_neural_foraminal_narrowing_l1_l2   0.242801  \n",
       "44036939_right_neural_foraminal_narrowing_l1_l2  0.277795  \n",
       "44036939_left_subarticular_stenosis_l1_l2        0.388988  \n",
       "44036939_right_subarticular_stenosis_l1_l2       0.426732  \n",
       "44036939_spinal_canal_stenosis_l2_l3             0.322521  \n",
       "44036939_left_neural_foraminal_narrowing_l2_l3   0.279294  \n",
       "44036939_right_neural_foraminal_narrowing_l2_l3  0.306953  \n",
       "44036939_left_subarticular_stenosis_l2_l3        0.394982  \n",
       "44036939_right_subarticular_stenosis_l2_l3       0.424978  \n",
       "44036939_spinal_canal_stenosis_l3_l4             0.337341  \n",
       "44036939_left_neural_foraminal_narrowing_l3_l4   0.288482  \n",
       "44036939_right_neural_foraminal_narrowing_l3_l4  0.321059  \n",
       "44036939_left_subarticular_stenosis_l3_l4        0.413648  \n",
       "44036939_right_subarticular_stenosis_l3_l4       0.450749  \n",
       "44036939_spinal_canal_stenosis_l4_l5             0.341640  \n",
       "44036939_left_neural_foraminal_narrowing_l4_l5   0.297436  \n",
       "44036939_right_neural_foraminal_narrowing_l4_l5  0.326949  \n",
       "44036939_left_subarticular_stenosis_l4_l5        0.424469  \n",
       "44036939_right_subarticular_stenosis_l4_l5       0.454337  \n",
       "44036939_spinal_canal_stenosis_l5_s1             0.321138  \n",
       "44036939_left_neural_foraminal_narrowing_l5_s1   0.301547  \n",
       "44036939_right_neural_foraminal_narrowing_l5_s1  0.323742  \n",
       "44036939_left_subarticular_stenosis_l5_s1        0.390498  \n",
       "44036939_right_subarticular_stenosis_l5_s1       0.426770  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97527f8b-489a-4d72-937f-beafd492587e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.48240703, 0.20657907, 0.31101385],\n",
       "        [0.34006482, 0.41713405, 0.2428011 ],\n",
       "        [0.32950148, 0.39270347, 0.27779502],\n",
       "        [0.27274927, 0.33826306, 0.3889877 ],\n",
       "        [0.25749156, 0.31577626, 0.4267322 ]], dtype=float32),\n",
       " array([[0.45364544, 0.22383353, 0.322521  ],\n",
       "        [0.304526  , 0.41618007, 0.2792939 ],\n",
       "        [0.2998361 , 0.39321098, 0.30695292],\n",
       "        [0.262683  , 0.34233522, 0.39498177],\n",
       "        [0.25024068, 0.32478124, 0.42497802]], dtype=float32),\n",
       " array([[0.44425666, 0.21840213, 0.33734122],\n",
       "        [0.28201714, 0.42950064, 0.2884822 ],\n",
       "        [0.2768153 , 0.40212578, 0.321059  ],\n",
       "        [0.23646207, 0.34988976, 0.41364822],\n",
       "        [0.22401266, 0.32523805, 0.45074928]], dtype=float32),\n",
       " array([[0.43976662, 0.21859288, 0.3416404 ],\n",
       "        [0.26724026, 0.43532407, 0.29743564],\n",
       "        [0.26477334, 0.40827805, 0.3269486 ],\n",
       "        [0.23020862, 0.34532288, 0.42446855],\n",
       "        [0.21990104, 0.3257624 , 0.45433658]], dtype=float32),\n",
       " array([[0.45570907, 0.22315317, 0.3211377 ],\n",
       "        [0.28407454, 0.41437867, 0.3015468 ],\n",
       "        [0.27923226, 0.39702538, 0.3237424 ],\n",
       "        [0.25616276, 0.3533389 , 0.39049834],\n",
       "        [0.24567866, 0.32755184, 0.42676952]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a3e08-737f-4e64-9658-2c7ea4a6347d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
