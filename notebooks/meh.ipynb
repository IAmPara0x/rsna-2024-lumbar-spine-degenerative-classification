{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31d3e51-f7c5-4ea4-8c25-ec9ccb4276be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "import time\n",
    "\n",
    "from lib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4558ebb-a49a-4db5-a3f9-faa513353c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Augmentation config\n",
    "AUG = True\n",
    "AUG_PROB = 0.75\n",
    "\n",
    "IMG_SIZE = [512, 512]\n",
    "MAX_SEQ_IMGS = 32\n",
    "\n",
    "N_LABELS = 25\n",
    "N_CLASSES = 3 * N_LABELS\n",
    "\n",
    "N_FOLDS = 5\n",
    "EPOCHS = 5 \n",
    "\n",
    "SEED = 8620\n",
    "N_WORKERS=4\n",
    "\n",
    "# Model Config\n",
    "OUTPUT_DIR=\"models\"\n",
    "MODEL_NAME = \"tf_efficientnet_b3.ns_jft_in1k\"\n",
    "\n",
    "# Device Config\n",
    "USE_AMP = True\n",
    "DEVICE=\"cuda:0\"\n",
    "\n",
    "# BATCH SIZE\n",
    "\n",
    "TGT_BATCH_SIZE = 32\n",
    "GRAD_ACC = 8\n",
    "BATCH_SIZE = TGT_BATCH_SIZE // GRAD_ACC\n",
    "# MAX_GRAD_NORM = None\n",
    "# EARLY_STOPPING_EPOCH = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9e0e72-f9e8-4ff5-83c0-7e97871e47c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3908/383465734.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(label2id)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.fillna(-100)\n",
    "label2id = {'Normal/Mild': 0, 'Moderate': 1, 'Severe':2}\n",
    "df = df.replace(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7286891a-eed8-498f-b525-846a7c2e0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA24Dataset(Dataset):\n",
    "    def __init__(self, df, phase='train', transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = np.zeros((512, 512, MAX_SEQ_IMGS), dtype=np.uint8)\n",
    "\n",
    "        # 4 = Patient pos (x,y,z) + slice location\n",
    "        pos_x = np.zeros((MAX_SEQ_IMGS, 3))\n",
    "\n",
    "        patient_id = self.df.iloc[idx][\"study_id\"]\n",
    "        label = self.df.iloc[idx][1:].values.astype(np.int64)\n",
    "        patient_info_path = f\"./processed-dataset/{patient_id}.pkl\"\n",
    "\n",
    "        with open(patient_info_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        scans_used = []\n",
    "\n",
    "        for series_info in data[\"series\"].values():\n",
    "\n",
    "            scan_type = series_info[\"series_description\"]\n",
    "\n",
    "            if scan_type in scans_used:\n",
    "                continue\n",
    "\n",
    "            scans = series_info[\"images\"]\n",
    "\n",
    "            if scan_type == \"Sagittal T2/STIR\":\n",
    "\n",
    "                for i in range(min(10, len(scans))):\n",
    "                    x[..., i] = scans[i][\"img\"].astype(np.uint8)\n",
    "                    pos_x[i] = np.array(list(scans[i][\"position_patient\"]))\n",
    "\n",
    "            elif scan_type == \"Sagittal T1\":\n",
    "\n",
    "                for i in range(min(10, len(scans))):\n",
    "                    x[..., i+10] = scans[i][\"img\"].astype(np.uint8)\n",
    "                    pos_x[i+10] = np.array(list(scans[i][\"position_patient\"]))\n",
    "\n",
    "            elif scan_type == \"Axial T2\":\n",
    "\n",
    "                for i in range(min(12,len(scans))):\n",
    "                    x[..., i+20] = scans[i][\"img\"].astype(np.uint8)\n",
    "                    pos_x[i+20] = np.array(list(scans[i][\"position_patient\"]))\n",
    "            else:\n",
    "                raise ValueError(f\"unknown series_description: {series_info[\"series_description\"]}\")\n",
    "\n",
    "            scans_used.append(scan_type)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(image=x)['image']\n",
    "\n",
    "        x = x.transpose(2, 0, 1)\n",
    "\n",
    "        pos_x = pos_x.astype(np.float32)\n",
    "        pos_x = pos_x - pos_x.min(0)\n",
    "\n",
    "        return (x, pos_x), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e12d55-510f-4b55-9456-b28cae51f0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paradox/.local/lib/python3.12/site-packages/pydantic/main.py:193: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "transforms_train = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=AUG_PROB),\n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5),\n",
    "        A.MedianBlur(blur_limit=5),\n",
    "        A.GaussianBlur(blur_limit=5),\n",
    "        A.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "    ], p=AUG_PROB),\n",
    "\n",
    "    A.OneOf([\n",
    "        A.OpticalDistortion(distort_limit=1.0),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "        A.ElasticTransform(alpha=3),\n",
    "    ], p=AUG_PROB),\n",
    "\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=AUG_PROB),\n",
    "    A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    A.CoarseDropout(max_holes=16, max_height=64, max_width=64, min_holes=1, min_height=8, min_width=8, p=AUG_PROB),    \n",
    "    A.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "transforms_val = A.Compose([\n",
    "    A.Resize(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    A.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccaf3333-ffad-4457-ba6c-87a92310cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, hidden_dim=1536, num_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.ln_2 = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        self.key_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.query_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, bias=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x, x_pos):\n",
    "        x = self.ln_1(x)\n",
    "\n",
    "        q,k,v = self.query_proj(x), self.key_proj(x), self.value_proj(x)\n",
    "        x = x + self.dropout(self.mha(q,k,v, need_weights=False)[0])\n",
    "        \n",
    "        x = self.ln_2(x)\n",
    "        x = x + self.dropout(self.mlp(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class RSNA24Model(nn.Module):\n",
    "    def __init__(self, model_name, n_classes=0, pos_in_features=3, blocks=2, pretrained=True, features_only=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_in_features = pos_in_features\n",
    "        self.embed_dim = 768\n",
    "\n",
    "        self.feature_extractor =  timm.create_model( model_name\n",
    "                              , pretrained=pretrained\n",
    "                              , num_classes=n_classes\n",
    "                              , global_pool='avg' \n",
    "                              , features_only=features_only\n",
    "                             )\n",
    "\n",
    "        self.conv_stem = nn.Sequential( nn.Conv2d(1,3, kernel_size=(1,1))\n",
    "                                                  \n",
    "                                      # NOTE: Try with activation\n",
    "                                      # , stride=(1,1)) ,nn.SiLU(inplace=True) \n",
    "                                      )\n",
    "\n",
    "        self.img_feature_dim  = 1536\n",
    "        \n",
    "        self.pos_embedding = nn.Sequential(\n",
    "            nn.Linear(pos_in_features, self.embed_dim),\n",
    "            )\n",
    "        self.img_feature_proj = nn.Linear(self.img_feature_dim, self.embed_dim)\n",
    "        \n",
    "        self.encoders = nn.ModuleList([Encoder() for _ in range(blocks)])\n",
    "        self.classifier = nn.Linear(self.embed_dim, N_CLASSES)\n",
    "\n",
    "\n",
    "        # NOTE: This is idea for combining kernel weights of the first conv layer\n",
    "        # self.feature_extractor.conv_stem.weight = nn.Parameter(self.feature_extractor.conv_stem.weight.sum(dim=1, keepdim=True))\n",
    "        # conv1_weight = self.feature_extractor.state_dict['conv_stem']\n",
    "        # self.feature_extractor.state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def forward(self, x, x_pos, freeze_conv=True):\n",
    "\n",
    "        batch_size, seq_len, h, w = x.shape\n",
    "\n",
    "        # x_pos: BATCH_SIZE, NUMBER OF SCANS, SCAN POSITIONS\n",
    "\n",
    "        x = x.view(-1, 1, h, w)\n",
    "        x = self.conv_stem(x)\n",
    "        if freeze_conv:\n",
    "            with torch.no_grad(): x = self.feature_extractor(x)\n",
    "        else:\n",
    "\n",
    "            n = 4\n",
    "            with torch.no_grad():\n",
    "                x = self.feature_extractor.conv_stem(x)\n",
    "                x = self.feature_extractor.bn1(x)\n",
    "\n",
    "                for i in range(n):\n",
    "                    x = self.feature_extractor.blocks[i](x)\n",
    "\n",
    "            for i in range(n,7):\n",
    "                x = self.feature_extractor.blocks[i](x)\n",
    "            x = self.feature_extractor.conv_head(x)\n",
    "            x = self.feature_extractor.bn2(x)\n",
    "            x = self.feature_extractor.global_pool(x)\n",
    "            x = self.feature_extractor.classifier(x)\n",
    "\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # x_pos = self.pos_embedding(x_pos)\n",
    "\n",
    "        x = self.img_feature_proj(x) + self.pos_embedding(x_pos)\n",
    "\n",
    "        for enc in self.encoders:\n",
    "            x = enc(x, None)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9cd0ab-7e58-4924-8446-2676cfd8b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "skf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d606db-ce06-459f-9da4-3dbd75357553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dl, model, optimizer, criterion, freeze=True):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    training_loss = []\n",
    "        \n",
    "    for idx, (batch_input, y) in enumerate(pbar := tqdm(train_dl)):\n",
    "        \n",
    "        x,x_pos = batch_input[0].to(DEVICE), batch_input[1].to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            pred = model(x,x_pos, freeze)[:, 0, :]\n",
    "            pred = pred.reshape(-1, 3)\n",
    "            y = y.view(-1)\n",
    "            loss = criterion(pred,y) / GRAD_ACC\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        training_loss.append(loss.item() * GRAD_ACC)\n",
    "\n",
    "        if (idx + 1) % GRAD_ACC == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        pbar.set_description(f\"loss: {loss.item() * GRAD_ACC:.6f}, training_loss: {np.mean(training_loss):.6f}\")\n",
    "                \n",
    "def validation(val_dl, model, criterion):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        y_preds = []\n",
    "        labels = []\n",
    "        avg_val_loss = []\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            for idx, (batch_input, y) in enumerate(pbar := tqdm(val_dl)):\n",
    "                \n",
    "                x,x_pos = batch_input[0].to(DEVICE), batch_input[1].to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "    \n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    pred = model(x,x_pos)[:, 0, :]\n",
    "                    pred = pred.reshape(-1, 3)\n",
    "                    y = y.view(-1)\n",
    "                    loss = criterion(pred,y)\n",
    "    \n",
    "    \n",
    "                y_preds.append(pred)\n",
    "                labels.append(y)\n",
    "    \n",
    "                avg_val_loss.append(loss.item())\n",
    "    \n",
    "                pbar.set_description(f\"loss: {loss.item():.6f}, validation_loss: {np.mean(avg_val_loss):.6f}\")\n",
    "    \n",
    "        y_preds = torch.cat(y_preds, dim=0)\n",
    "        labels = torch.cat(labels)\n",
    "    \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            final_val_loss = criterion(y_preds, labels).item()\n",
    "            print(f\"Final loss on validation set: {final_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12bf10cc-6ce0-4e41-bfb8-93b070293b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/tf_efficientnet_b3.ns_jft_in1k)\n",
      "INFO:timm.models._hub:[timm/tf_efficientnet_b3.ns_jft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.922102, training_loss: 0.887149: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 395/395 [02:22<00:00,  2.77it/s]\n",
      "loss: 0.661901, validation_loss: 0.842632: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 99/99 [00:18<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss on validation set: 0.862793\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.751310, training_loss: 0.799280: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 395/395 [02:22<00:00,  2.78it/s]\n",
      "loss: 0.740905, validation_loss: 0.769233: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 99/99 [00:18<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss on validation set: 0.784941\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.925840, training_loss: 0.754734: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 395/395 [02:22<00:00,  2.78it/s]\n",
      "loss: 0.585909, validation_loss: 0.726168: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 99/99 [00:18<00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss on validation set: 0.742697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(range(len(df)))):\n",
    "\n",
    "    model = RSNA24Model(MODEL_NAME).to(DEVICE)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    weights = torch.tensor([1.0, 2.0, 4.0])\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(DEVICE))\n",
    "\n",
    "    df_train = df.iloc[trn_idx]\n",
    "    df_valid = df.iloc[val_idx]\n",
    "    \n",
    "    train_ds = RSNA24Dataset(df_train, transform=transforms_train)\n",
    "    train_dl = DataLoader(\n",
    "                train_ds,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                drop_last=True,\n",
    "                num_workers=N_WORKERS\n",
    "                )\n",
    "    \n",
    "    valid_ds = RSNA24Dataset(df_valid, transform=transforms_val)\n",
    "    valid_dl = DataLoader(\n",
    "                    valid_ds,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    drop_last=False,\n",
    "                    num_workers=N_WORKERS\n",
    "                    )\n",
    "\n",
    "    for i in range(3):\n",
    "        print(f\"EPOCH: {i + 1}\")\n",
    "\n",
    "        # if i == 0:\n",
    "        #     train(train_dl, model, optimizer, criterion, freeze=True)\n",
    "        # else:\n",
    "        train(train_dl, model, optimizer, criterion, freeze=False)\n",
    "        validation(valid_dl, model, criterion)\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed21686-8bea-4c84-a034-9dd88ea80eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551a3bc-375a-49b7-ac2f-ab851d8f7302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
